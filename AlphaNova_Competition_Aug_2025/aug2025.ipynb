{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26da59e2-edbd-42a7-a3f8-24ef336898be",
   "metadata": {},
   "source": [
    "### Competition Title: Competition August 2025\n",
    "\n",
    "#### Final Submission Deadline:  \n",
    "\n",
    "You will have until 23:59 UTC on October 15 2025 to submit your entries. You may submit up to 5 entries\n",
    "\n",
    "#### Prize\n",
    "\n",
    "Subject to the Contest Rules, the winner of this competition will receive a cash prize of $5000 USD, paid either by stablecoins or bank transfer.  \n",
    "\n",
    "#### Profit Sharing Opportunity\n",
    "\n",
    "A selection of top candidate models will also be eligible for future performance fee sharing, based on picking top representatives from a correlation clustering analysis.  Details on this to follow.\n",
    "\n",
    "#### Objective\n",
    "Participants are challenged to develop a predictive a cross-sectional signal that forecasts the cross-sectional returns of a set of assets. The goal is to create a signal $P(i)$ at each timestamp $i$ that effectively predicts a cross-sectional target return. Predictive quality is normally in this case measured by time averaged cross-sectional correlation. However, the objective is to maximize a utility function $U$, which is in fact strongly linked to maximizing time averaged cross-sectional correlation.\n",
    "\n",
    "This competition introduces **automated validation** - your solution must be structured in a way that allows it to be executed, trained, and validated automatically by our runner code.\n",
    "\n",
    "#### Background\n",
    "Financial markets are known for their nonstationarity and complex dynamics. The assets in this contest are popular instruments that allow one to go long or short with leverage.  Our core activity is trading baskets of these instruments on a long short market neutral basis. This challenge tests participants' ability to forecast returns that are not only accurate but also consistent over time.\n",
    "\n",
    "#### Competition Data\n",
    "\n",
    "The Competition Data consists of:\n",
    "a) 41 cross-sectional features for 20 assets, returns, and cross-sectional targets.\n",
    "b) A prototype Jupyter notebook that provides an example implementation.\n",
    "c) Helper modules for data loading, evaluation, and automated execution of code.\n",
    "\n",
    "Within the notebook and helper modules, you will find a Predictor class interface. Your submission must strictly adhere to this interface. It should be a subclass of the Predictor class. While you are allowed to add new methods, you cannot modify or remove any existing method signatures.  You may not modify any .py files as well.  Do not define helper functions outside of your subclass. All functions should be contained in your subclass of Predictor as methods.\n",
    "\n",
    "Additionally, you must partition the historical data into training and validation sets using the exact method specified in the notebook. Please note that in this contest, the test set will be based on a combination of historical data and live simulation conducted over the Testing Period.\n",
    "\n",
    "#### Historical Data Description\n",
    "- **Returns:** A time series of returns on assets, $X(i) = (X_1(i), X_2(i), ..., X_J(i))$, where $i$ denotes the time index expressed as an integer and each $X_j(i)$ represents the one step return of the $j$-th asset at time $i$.  This data is used for backtesting and computing the Utility U of the signal.\n",
    "\n",
    "- **Features:** In one dataframe, we provide 41 cross-sectional features on 20 assets. This dataframe has columns that are multi-indexed with zero level being feature name and first level being the ticker. Each timestamp $i$ is associated with, essentially, for each feature (i.e. Feature.12), a vector of dimension J of values for that feature for each ticker. Any of this data can be used as features.  Similarly, the returns can be used as features, as long as they are not shifted by a negative integer.\n",
    "\n",
    "- **Target Returns:** The data is a cross-sectionally z-scored time series of of h-forward returns.  Do not shift this forward or backward.\n",
    "\n",
    "#### Signal Construction.\n",
    "\n",
    "- participants will create a signal that is cross-sectionally z-scored.  $P(i) = (P_1(i), P_2(i), ..., P_J(i))$,\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{J} P_j(i) = 0, \\quad \\frac{1}{J} \\sum_{j=1}^{J} P_j(i)^2 = 1\n",
    "$$\n",
    "\n",
    "This is a requirement for our purposes.  It also forces the signal to be \"dollar neutral\", if it were treated as a sole 'trading strategy' (but to be clear, it won't be used in this manner, if it is used at all).  Signals that are not cross-sectionally Z-scored will be disqualified.  We note that this is a cross-sectional signals that should be predictive of cross-sectional returns.  the degree of predictability will clearly relate to quality of Utility function output.\n",
    "\n",
    "You may use Returns and any data in Features or any transformations of them to build your signal. But, whatever you do in your transformation, DO NOT peer into the future - Data Leakage will be automatically detected and such submissions will be disqualified. An example of a disallowed transformation would be shifting the returns back by n steps.  That is data leakage.\n",
    "\n",
    "#### Utility  Function $U$\n",
    "The performance of each submission will be evaluated using the following utility function:\n",
    "\n",
    "##### Sharpe ratio\n",
    "\n",
    "The net return at time $i$ is defined as:\n",
    "\n",
    "$$\n",
    "r(i) = \\langle P(i), X(i+1) \\rangle\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X(i+1)$ is the forward return vector from $i$ to $i+1$\n",
    "- $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product across assets\n",
    "\n",
    "The utility function $U$ is then:\n",
    "\n",
    "$$\n",
    "U = \\frac{\\mathbb{E}[r(i)]}{\\sqrt{\\text{Var}[r(i)]}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Scoring Criteria\n",
    "\n",
    "Signals generated by models that AlphaNova determines to be overfit will be disqualified. Overfitting occurs when a model performs well on historical data but fails to generalize to new data. We encourage you to conduct your own overfitting tests before submitting your model, but do NOT include your overfitting test in the submission- it will run too slowly if you do.\n",
    "\n",
    "AlphaNova will automatically test each submission for overfitting using a Monte Carlo simulation. This approach evaluates the model's training method by applying it to randomly generated data samples. The key metric is the 95% quantile Sharpe Ratio, which represents the  Sharpe Ratio such that higher Sharpe Ratios are in the 95% quantile.\n",
    "\n",
    "If a model's actual Loss Function value is lower than this threshold, we cannot be confident that the model is not simply fitting historical data. As a result, the model will be disqualified.\n",
    "\n",
    "Participants submissions that aren't deemed to be overfit will be ranked based on the Sharpe Ratio  $U$ in a live testing period.\n",
    "\n",
    "#### Submission Format\n",
    "\n",
    "**IMPORTANT: This contest introduces automated validation. Your submission must be structured to allow automated execution.**\n",
    "\n",
    "Your submission should be either:\n",
    "\n",
    "1. This notebook (aug2025.ipynb) with your Predictor implementation.\n",
    "2. A .py file containing your Predictor class implementation with necessary imports.\n",
    "\n",
    "##### Naming Your Submission\n",
    "Feel free to name your submission files as you wish:\n",
    "- name.ipynb (for notebooks)\n",
    "- name.py (for Python scripts)\n",
    "\n",
    "##### Implementing Your Predictor\n",
    "Your implementation must:\n",
    "1. Inherit from the base Predictor class defined in predictor.py file\n",
    "2. Implement all required methods (train, predict)\n",
    "3. Contain all necessary code within the Predictor class, not in functions outside of the class\n",
    "4. Be self-contained or properly import all dependencies. Use PEP 723 format to specify dependencies.\n",
    "5. Not modify the method signatures\n",
    "\n",
    "##### Automated Validation Structure\n",
    "Your code will be validated using:\n",
    "1. **runner.py** - Automated execution script\n",
    "2. **evaluation.py** - Scoring and validation functions\n",
    "3. **data_loader.py** - Data loading utilities\n",
    "\n",
    "Each submission will be:\n",
    "1. Automatically loaded and instantiated\n",
    "2. Trained on the training set\n",
    "3. Evaluated on validation\n",
    "4. Final evaluation during the test period\n",
    "5. Scored using the utility function  $U$\n",
    "\n",
    "##### Submission\n",
    "Once your submission is complete, upload solution file at the competition page on www.alphanova.tech.\n",
    "\n",
    "#### Computation Rules and Guidelines\n",
    "- **NO GPU**:  AlphaNova at this time will not accept models that effectively require GPU for training.\n",
    "- **Training Time**: Training should complete within 15 minutes on a standard CPU instance.\n",
    "- **Forecast Time**: Individual predictions should complete within 60 seconds per timestamp.\n",
    "- **Memory Usage**: Your solution should not exceed 8GB of RAM during execution.\n",
    "\n",
    "#### Winning Criteria\n",
    "The participant whose submission is not overfit and with the highest $U$ over the Test Period will be declared the winner. In the event that multiple users by chance have the same winning utility, we will reward the full prize to the contestant who submitted his/her solution *first*.\n",
    "\n",
    "#### Test Period\n",
    "The Test Period will be composed of a hidden historical period plus 2 months of live similation. we will endeavour to start the live simulation within 10 working days of the contest deadline.  Participants will be able to track the performance of their signals on a dashboard at a frequency that will be disclosed in the coming weeks.  All signals that are not deemed to be overfit and have sufficiently positive Sharpe Ratio during the test period will be eligible to be onboarded into our trading system, conditioned on being best representatives in a correlation clustering analysis.  Details on this will be disclosed in the coming weeks.\n",
    "\n",
    "#### Additional Notes\n",
    "- **Data Obfuscation**: The data is obfuscated.  In the unlikely event that you can reverse engineer the data, it will most likely be flagged by the unusually high quality of your validation sharpe, as well as the sharpe ratio of the historical portion of the data set. Any submission that has unusually large sharpe on validation as well as in the historical component of the Test Period will be disqualified.\n",
    "- **Automated Execution**: Your code must be executable without manual intervention.\n",
    "- **Dependencies**: All required packages must be standard Python libraries or clearly specified.\n",
    "\n",
    "#### Important Note for the avoidance of doubt\n",
    "You must only modify:\n",
    "1. The Predictor class implementation (constructor, train, and predict methods)\n",
    "2. Any dependencies you need to add\n",
    "3. Import statements as needed\n",
    "\n",
    "Do not modify:\n",
    "- The base Predictor interface\n",
    "- Data loading procedures\n",
    "- Evaluation metrics\n",
    "- The train/validation split methodology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455d6d7-ca89-4e0b-8fcd-f5e5c779dbde",
   "metadata": {},
   "source": [
    "# Competition August 2025: Automated Validation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0gknfrdvs",
   "metadata": {},
   "source": [
    "## Automated Validation Instructions\n",
    "\n",
    "This competition uses automated validation. Your submission will be:\n",
    "1. Loaded by the runner script\n",
    "2. Instantiated using constructor\n",
    "3. Trained on the training data\n",
    "4. Evaluated on validation and test data\n",
    "\n",
    "To test your submission locally, use:\n",
    "```bash\n",
    "python runner.py your_submission.ipynb\n",
    "# or\n",
    "python runner.py your_submission.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d26f9c-2df4-4aa6-a606-8258d0c01119",
   "metadata": {},
   "source": [
    "## Example of adding necessary dependencies\n",
    "It works with both pip and uv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df0064-a562-4134-baf7-4e855d289b35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:38.005783Z",
     "start_time": "2025-08-12T18:37:38.003792Z"
    }
   },
   "outputs": [],
   "source": [
    "!uv pip install xgboost 2>/dev/null || pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e7dea-1994-42cd-85fc-3c52d2f49ea0",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The competition uses the same data format as Contest #3. \n",
    "All data loading is now handled by the `data_loader.py` module for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f8bd6-ae84-42e2-b94d-f29d07e4e85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:38.381203Z",
     "start_time": "2025-08-12T18:37:38.068450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data loading functions\n",
    "from data_loader import load_data, split_data\n",
    "\n",
    "# Load competition data\n",
    "returns, features, target_returns = load_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe7505-a0b8-45fb-b683-24a29ee38f72",
   "metadata": {},
   "source": [
    "## Train/Validation Split\n",
    "\n",
    "The split is performed automatically by the data loader to ensure consistency across all submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ca0ee-0346-45ca-b029-8c1a47be8ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:38.435640Z",
     "start_time": "2025-08-12T18:37:38.386683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "# IMPORTANT: Always use test_size=0.25 as specified\n",
    "train_data, validate_data = split_data(returns, features, target_returns, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcc00b-995b-4175-9db0-3ab47b9d3750",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Evaluation functions are provided in the `evaluation.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbaea9-b627-4b5a-92eb-0713c7ef0b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:38.449618Z",
     "start_time": "2025-08-12T18:37:38.447706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import evaluation functions\n",
    "from evaluation import backtest, returns_to_equity, sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04739e0-2f64-4417-9f9d-8a5594722055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:38.505330Z",
     "start_time": "2025-08-12T18:37:38.499667Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from predictor import Predictor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class XGBoostPredictor(Predictor):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.asset_list = []\n",
    "        self.feature_names = []\n",
    "        self.trained = False\n",
    "        self.params = {\n",
    "            \"max_depth\": 5,\n",
    "            \"eta\": 0.1,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def cross_sectional_corr_objective(self, y_pred: np.ndarray, dtrain: xgb.DMatrix):\n",
    "        \"\"\"\n",
    "        Custom XGBoost objective: maximize cross-sectional correlation between predictions\n",
    "        and targets (no turnover penalty).\n",
    "        \"\"\"\n",
    "        y_true = dtrain.get_label()\n",
    "\n",
    "        n_assets = len(self.asset_list)\n",
    "\n",
    "        # Reshape to (T, N)\n",
    "        y_true = y_true.reshape(-1, n_assets)\n",
    "        y_pred = y_pred.reshape(-1, n_assets)\n",
    "\n",
    "        # Cross-sectional centering\n",
    "        r_centered = y_true - y_true.mean(axis=1, keepdims=True)\n",
    "\n",
    "        # Gradient = negative centered returns (maximize correlation)\n",
    "        grad = -r_centered.ravel()\n",
    "\n",
    "        # Hessian = constant\n",
    "        hess = np.ones_like(grad)\n",
    "\n",
    "        return grad, hess\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def train(self, features: pd.DataFrame, target: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Train XGBoost model with custom cross-sectional correlation objective.\n",
    "        \"\"\"\n",
    "        # Transform features\n",
    "        self.asset_list = features.columns.levels[1].tolist()\n",
    "        self.feature_names = features.columns.levels[0].tolist()\n",
    "\n",
    "        # Prepare stacked data\n",
    "        X = features.stack(level=1)\n",
    "        y = target.stack()\n",
    "        mask = ~y.isna()\n",
    "\n",
    "        X = X.loc[mask]\n",
    "        y = y.loc[mask]\n",
    "\n",
    "        # Convert to XGBoost DMatrix\n",
    "        dtrain = xgb.DMatrix(X.values, label=y.values)\n",
    "\n",
    "        # Train with custom objective\n",
    "        self.model = xgb.train(\n",
    "            self.params,\n",
    "            dtrain,\n",
    "            num_boost_round=self.params.get(\"n_estimators\", 50),\n",
    "            obj=self.cross_sectional_corr_objective,\n",
    "        )\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def predict(self, features: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate cross-sectionally z-scored predictions for each timestamp.\n",
    "        \"\"\"\n",
    "        assert self.trained, \"Model must be trained before predicting.\"\n",
    "\n",
    "        # Transform features\n",
    "        index = features.index\n",
    "        X = features.stack(level=1)\n",
    "\n",
    "        # Predict\n",
    "        preds = self.model.predict(xgb.DMatrix(X.values))\n",
    "\n",
    "        # Reshape to (time, asset)\n",
    "        pred_matrix = pd.DataFrame(preds.reshape(-1, len(self.asset_list)), index=index, columns=self.asset_list)\n",
    "\n",
    "        # Cross-sectional z-score at each timestamp\n",
    "\n",
    "        mean_vals = pred_matrix.mean(axis=1)\n",
    "        std_vals = pred_matrix.std(axis=1).replace(0, 1)  # avoid division by zero\n",
    "        pred_matrix = pred_matrix.sub(mean_vals, axis=0).div(std_vals, axis=0).fillna(0.0)\n",
    "        pred_matrix = pred_matrix.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "        return pred_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0d4d-afa0-4626-8805-15eff392905b",
   "metadata": {},
   "source": [
    "## Training Example\n",
    "\n",
    "Below is an example of training and evaluating a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf06c1f-0501-4ca9-8554-3bd1cd3236b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:39.912388Z",
     "start_time": "2025-08-12T18:37:38.555304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and train the example predictor\n",
    "predictor = XGBoostPredictor()\n",
    "predictor.train(train_data[\"features\"], train_data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff62a5-00fe-4705-bfb4-cedf55e0ee2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.305933Z",
     "start_time": "2025-08-12T18:37:39.966072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on training data\n",
    "train_predictions = predictor.predict(train_data[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd030c4-24de-414e-a3d5-dac9e29023fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.328671Z",
     "start_time": "2025-08-12T18:37:40.320397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate training Sharpe\n",
    "sharpe = sharpe_ratio(train_predictions, train_data[\"returns\"])\n",
    "print(sharpe)\n",
    "print(f\"Training Sharpe: {sharpe:.20f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c9bf-d49c-4bb3-bf34-52e3f3296ebb",
   "metadata": {},
   "source": [
    "### Backtest Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a91ae2-8596-4f0c-af27-fb5974ed613f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.456669Z",
     "start_time": "2025-08-12T18:37:40.373543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize backtest results (optional, not used for scoring)\n",
    "pf_returns = backtest(train_predictions, train_data[\"returns\"])\n",
    "# pf_returns=backtest(train_predictions, train_data['returns'])\n",
    "returns_to_equity(pf_returns).plot(title=\"Training Backtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c1e10-5826-48cf-a8fb-b8c96632ac36",
   "metadata": {},
   "source": [
    "## Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e7a41-b5f5-4207-9838-a9ca80bd4d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.555612Z",
     "start_time": "2025-08-12T18:37:40.468844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "validate_predictions = predictor.predict(validate_data[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a712a89-584d-41da-9f28-7a812aa7e2ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.574402Z",
     "start_time": "2025-08-12T18:37:40.569123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation Sharpe\n",
    "sharpe = sharpe_ratio(validate_predictions, validate_data[\"returns\"])\n",
    "print(sharpe)\n",
    "print(f\"Validation Sharpe: {sharpe:.20f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831750d6-4b93-450b-96a0-4759fd317914",
   "metadata": {},
   "source": [
    "### Validation Backtest (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76959493-0a88-45f9-b953-a31c926b0a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:40.700446Z",
     "start_time": "2025-08-12T18:37:40.621029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize validation backtest (optional)\n",
    "validate_pf_returns = backtest(validate_predictions, validate_data[\"returns\"])\n",
    "returns_to_equity(validate_pf_returns).plot(title=\"Validation Backtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de7qhezoi",
   "metadata": {},
   "source": [
    "## Automated Validation\n",
    "\n",
    "To validate your submission automatically, run:\n",
    "\n",
    "```bash\n",
    "python runner.py aug2025.ipynb\n",
    "or\n",
    "python runner.py sample_submission.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Load your predictor class\n",
    "2. Train it on the training data\n",
    "3. Evaluate it on training and validation sets\n",
    "4. Save results to `results.csv`\n",
    "\n",
    "The runner enforces time limits:\n",
    "- Training: 15 minutes maximum\n",
    "- Prediction: 60 second per call maximum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
